Sean Ventrella
CSE 446/Assignment #1
Question Responses

**Code Description**
	My code was written in Java.
	----------------------------

	(1) First the code parses the data for each dataset (training and test) into two
		seperate Sets of PageView objects (contains label value and array of features.
		Additionally we parse the feature name file into a string array for quick
		lookup capability.
		------------------------------------------------------------------------------
		Next we build the decision tree on the training dataset. This is done by
		calling the method "learnTree" which accepts a set of PageView objects, an
		array of feature names, a set of used feature indices, and the threshhold 
		value. The method starts by checking if we need to create a leaf node based on
		the current set having all positives or all negatives or all features have 
		been exhausted. 

		If a leaf node was not created then we compute the attribute containing the
		maximum information gain. We keep track of the attribute index to split on.
		The information gain is calculated in a method "informationGain" which
		uses the algorithm of Entropy(S) - sum of entropies for all feature values
		to calculate and return the gain. The entropies are calculated by calling the 
		method "entropy" which uses the equation in the textbook to properly
		compute the entropy for a given set of examples.

		Next we calculate the chi-square value to determine if we are under the 
		passed in accepted threshhold value. This is calculate by calling the method
		"chiSquare" which follows the algorithm stated in the specification document.

		If we are still above the accepted threshhold then we create a new DTreeNode
		and pass it the feature name we are splitting on, the feature index, the 
		default label, and a new hashmap to store the branches in for each value the
		feature can take on. Then for each possible value, we recursively call 
		"learnTree" on it and add that subtree to our map as a branch.
		-----------------------------------------------------------------------------
		Once the tree has been constructed we can now run the test data on it to 
		recieve predicitons on the output. Output is stored in a list and written to
		a file for easy diff(ing). 

		To predict we loop through each example in the test set and call the method
		"predictTree" which traverses the tree based on the feature being checked
		at each node of the tree until a leaf is found, at which point the label is
		returned and written to the output file called "clickstream_results.txt".
		
		After all predictions have been made the program prints out the tree size and
		calls "computeAccuracy" which calculates and prints to the console
		the number of correct matches predicted along with the percent accuracy.
		-----------------------------------------------------------------------------
		USAGE NOTES:
		When running be sure to increase the jvm heapspace size as this can be
		necessary for how many branches are created depending on the threshhold.
		Also pass in the command line aregument threshhold value or it will tell you
		to enter it in an error message and cease executing as a check I put in.
		Example Command Line:
		--------------------
		javac Clickstream.java
		java -Xmx3072m Clickstream 0.01

**Section 1.5 Responses**
1)
	.01 - Accuracy = 73.26% , Size = 459 (nodes+leaves)
	.05 - Accuracy = 73.06% , Size = 948 (nodes+leaves)
	1.0 - Accuracy = 65.17% , Size = 417644 (nodes+leaves)

	Observation: As we continue to restrict the threshhold, we improve our accuracy.
				 This is a substantial gain in accuracy and reduction in node
				 creation compared to the oversplitting we see when simply splitting
				 as much as possible. Unfortunately this still is not as good as 
				 simply choosing to predict 0 for every example in the set, which
				 would give a roughly 74.8% accuracy.
2) 
	Options
		- Through our Chi-Squared split-stopping, the 0.05 and 0.01 threshhold values
		  both substantially improved the predictive accuracy of our results. This
		  makes sense because we are essentially computing whether the information
		  gain from splitting is worth it to us. This helps by not overfitting the
		  training data and instead allow more flexibility in our predictions for
		  test data. THe split-stopping helps tp reduce the growth of our tree, 
		  benefiting both the predictive accuracy and the total tree size, and
		  increasing the Runtime of the overall algorithm. Using 0.01 still provided
		  even more improvement over 0.05 threshhold, indicating that our tree
		  still had room to improve on the degree of splitting.

3)
	Path: "Session Request Count" -> "Num checkout Template Views" -> "US State"

	This is one of the paths starting at the root, and this path makes sense because
	we should clearly expect the session count to be a large number to split on and
	then understanding how many people view the checkout page broken up by each state
	seems like a relative indicator of whether someone is staying on the site. For 
	instance, people in more rural areas are probably less likely to spend much time
	shopping online because rural communities tend to be agricultural/environment based
	where urban areas are probably likely to see larger population numbers indicating
	increased consumers performin online transactions for goods.

4) Extra Credit:
	If I had 10-20 different p-values to choose from I would certainly pick the one
	the gives the lowest testing error over the lowest training error. The reasoning
	is that as you reduce the erro of training data, you quickly induce over-splitting
	which although your training data predicitons look accurate, quickly results in
	test datasets to be predicted poorly with decreasing accuracy. Therefore you want
	to ensure that you choose a p-value that consistently results in the testing-data
	having the lowest possible error since this is what you will use when making real
	predicitons on large datasets. So you should choose choice (b).

**Section 2 Rule Induction**
	Number of Rules: 2^(d) rules.
	
	Number of Preconditions: d preconditions per rule.

	Number of distinct choices: d*2^(d) distinct choices. 

	More Prone to Overfitting: Most datasets will contain a fair amount of noise, which
							   affects the construction of fitting a hypothesis to the
							   data, so we should see overfitting occur in each algorithm.
							   That being said, a sequential covering algorithm such as
							   CN2 seems more likely to suffer from the affects of
							   overfitting than a decision tree method such as ID3. The 
							   reasoning behind this is that each rule in a ruleset only
							   spans one hypothesis, and each rule can range from 1 to
							   all possible attributes being combined. This means that a
							   ruleset will exhaust all possible permutations of the 
							   attributes. This varies from a decision tree in that
							   each layer of a decision tree essentially locks in a 
							   particular attribute, and therefore the number of permutations
							   we generate is greatly reduced from all possible. Essentially
							   we expect CN2 to create more rules for hypothesis than a
							   decision tree will.
